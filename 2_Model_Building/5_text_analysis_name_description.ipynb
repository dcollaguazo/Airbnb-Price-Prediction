{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# !{sys.executable} -m pip install -U spacy\n",
    "# !{sys.executable} -m pip install gensim\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "# !{sys.executable} -m pip install pyLDAvis\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install langdetect\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install yellowbrick\n",
    "# !{sys.executable} -m pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from yellowbrick.features import FeatureImportances\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how listing description influence price and listing rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AirBnb dataset provides with features not only that have numeric or structured values but also unstructured data. To mention a few examples: the name of of the property, a short description of the space, an overview of the neighborhood and a description of the listing itself. As part of our project we would like to use Natural Language Processing techniques to understand how unstructured data available for each listing might influence price.\n",
    "\n",
    "The reason we believe that there might be a relationship betweeen the listing description and the price is because \n",
    "we think that what makes a listing valuable is not only their features per se but also the way the place is presented to the potential guest. This, not only includes the pictures of the listing but also the words that the listing owners pick to describe their listing and assumingly seeks to attrack their potential customers. We assume that by including certain words in the description could potentially increase the attractiveness of a certain property to be rented. Thus, due to supply and demand law, can assume that the more demanded a property is, the higher the value it could have. \n",
    "\n",
    "To begin with the analysis we want to see if there are similarities in the way listing owners describe their place and the price they charge per night. We proceed with the assumption that listings with similar features will use similar words to describe their property. Furthermore we are curious about how words in the description could inform us about the listing's overall rating.\n",
    "\n",
    "For this reason we will be using this notebook to try to answer the following questions:\n",
    "\n",
    "1. Do listings using similar words in their description share a similar price?\n",
    "2. Do listings using similar words in their description share other features that are relevant to predict price?\n",
    "3. Do listings using similar words in their description end up having similar guest acceptance/rating?\n",
    "\n",
    "For this purpose we will first, use Latent Dirichlet Allocation to elicit the latent topics in the description. Latent Dirichlet Allocation is an unsupervised algorithm, largely used in Natural Language Processing to perform what is known as Topic Modeling. This algorightm uses probability distribution to discover which parts of the data are similar. In the context of our project we would try to discover the similar words used the description of New York City listings that could potentially influence price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/new-york-city-airbnb-open-data/'\n",
    "listings_csv = os.path.join(path,'listings.csv')\n",
    "\n",
    "listings_df =  pd.read_csv(listings_csv,low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating content column with name and description of property\n",
    "listings_df['content'] = listings_df['name'] + listings_df['description']\n",
    "content = listings_df[['id','content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be able to see, the description is the single text field with fewer than 3% missing values. This is also one of the reasons why we chose to work with this field to experiment with NLP into trying to predict price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_nulls = round((listings_df.summary.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "space = round((listings_df.space.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "description = round((listings_df.description.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "neighborhood_overview = round((listings_df.neighborhood_overview.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "notes = round((listings_df.notes.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "transit = round((listings_df.transit.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "access = round((listings_df.access.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "interaction = round((listings_df.interaction.isnull().sum()/listings_df.shape[0])*100, 1)\n",
    "house_rules = round((listings_df.house_rules.isnull().sum()/listings_df.shape[0])*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8% of listings have empty summaries\n",
      "28.0% of listings have empty space description\n",
      "2.4% of listings have empty description\n",
      "34.4% of listings have empty overviews on their neighborhood\n",
      "59.6% of listings have no notes from the host\n",
      "34.4% of listings have no information on transit\n",
      "47.2% of listings have no information on accessibility\n",
      "40.2% of listings have no information on interaction\n",
      "39.1% of listings have no information on house_rules\n"
     ]
    }
   ],
   "source": [
    "print(f\"{summary_nulls}% of listings have empty summaries\")\n",
    "print(f\"{space}% of listings have empty space description\")\n",
    "print(f\"{description}% of listings have empty description\")\n",
    "print(f\"{neighborhood_overview}% of listings have empty overviews on their neighborhood\")\n",
    "print(f\"{notes}% of listings have no notes from the host\")\n",
    "print(f\"{transit}% of listings have no information on transit\")\n",
    "print(f\"{access}% of listings have no information on accessibility\")\n",
    "print(f\"{interaction}% of listings have no information on interaction\")\n",
    "print(f\"{house_rules}% of listings have no information on house_rules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing numeric features from previous Exploratory Data Analysis and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model df\n",
    "model_cols_df = pd.read_csv('../data/new-york-city-airbnb-open-data/model_columns_listings.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_identity_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2595</td>\n",
       "      <td>733294</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3831</td>\n",
       "      <td>733383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5099</td>\n",
       "      <td>733440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5121</td>\n",
       "      <td>733441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5178</td>\n",
       "      <td>733469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  host_since  host_is_superhost  host_identity_verified\n",
       "0  2595      733294                  0                       1\n",
       "1  3831      733383                  0                       1\n",
       "2  5099      733440                  0                       0\n",
       "3  5121      733441                  0                       0\n",
       "4  5178      733469                  0                       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cols_df.iloc[:,0:4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the new created 'content' column with the model dataframe just to choose rows that are relevant\n",
    "df_model = pd.merge(left=model_cols_df, right=content, how='left', on='id')\n",
    "\n",
    "# dropping null values\n",
    "df_model = df_model.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_identity_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2595</td>\n",
       "      <td>733294</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3831</td>\n",
       "      <td>733383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5099</td>\n",
       "      <td>733440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5121</td>\n",
       "      <td>733441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5178</td>\n",
       "      <td>733469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  host_since  host_is_superhost  host_identity_verified\n",
       "0  2595      733294                  0                       1\n",
       "1  3831      733383                  0                       1\n",
       "2  5099      733440                  0                       0\n",
       "3  5121      733441                  0                       0\n",
       "4  5178      733469                  0                       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.iloc[:,0:4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Language from content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we may already know, AirBnb is a platform used by people from all around the world. Therefore, some hosts that want to attract guests who speak languages other than English might want to use different language or languages to describe their place. For our analysis we want to make sure that we are working with a single language and since the majority of reviews are written in English, we will use langdetect, to determine those descriptions written in English only. Langdetect is a language detection algorithm that supports 55 languages out of the box. To maximize the probability of the algorithms predicting the correct language we make sure we send it a long enough string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang(x):\n",
    "    lang=''\n",
    "    txt_len=len(x)\n",
    "    if txt_len>50:\n",
    "        try:\n",
    "            lang=detect(x)\n",
    "        except Exception as e:\n",
    "            lang=''\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['content_lang'] = df_model.content.apply(lambda x: predict_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting only text in English\n",
    "df_model_en_desc = df_model[df_model.content_lang=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27354, 47), (27108, 47))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.shape, df_model_en_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_identity_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2595</td>\n",
       "      <td>733294</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3831</td>\n",
       "      <td>733383</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5099</td>\n",
       "      <td>733440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5121</td>\n",
       "      <td>733441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5178</td>\n",
       "      <td>733469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  host_since  host_is_superhost  host_identity_verified\n",
       "0  2595      733294                  0                       1\n",
       "1  3831      733383                  0                       1\n",
       "2  5099      733440                  0                       0\n",
       "3  5121      733441                  0                       0\n",
       "4  5178      733469                  0                       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_en_desc.iloc[:,0:4].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA for Topic Modeling in Listing Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_content(df):\n",
    "#     df['content'] = df['name'] + df['summary']\n",
    "#     df['content'] = df['content'].replace(np.nan, '', regex=True)\n",
    "    \n",
    "    # Convert to list\n",
    "    data = df['content'].values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_texts = generate_list_content(df_model_en_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words and clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are tokenizing each document.\n",
    "def content_to_words(lst_texts):\n",
    "    for text in lst_texts:\n",
    "        yield(gensim.utils.simple_preprocess(str(text), deacc=True))\n",
    "        \n",
    "# data_words is a list where each element is the tokenized document\n",
    "tokenized_content = list(content_to_words(lst_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bigram and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_content, min_count=10, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tokenized_content], threshold=100)  \n",
    "\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(tokenized_content):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in tokenized_content]\n",
    "\n",
    "def make_bigrams(tokenized_content):\n",
    "    return [bigram_mod[doc] for doc in tokenized_content]\n",
    "\n",
    "def make_trigrams(tokenized_content):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in tokenized_content]\n",
    "\n",
    "def lemmatization(tokenized_content, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tokenized_content:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "tokenized_content_nostops = remove_stopwords(tokenized_content)\n",
    "\n",
    "# Form Bigrams\n",
    "tokenized_content_bigrams = make_bigrams(tokenized_content_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "tokenized_content_lemmatized = lemmatization(tokenized_content_bigrams, \n",
    "                                             allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary:\n",
    "# Mapping from word IDs to words. \n",
    "# It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "id2word = corpora.Dictionary(tokenized_content_lemmatized)\n",
    "# print(len(id2word)) # corpus has 14118 unique tokens\n",
    "\n",
    "# Term Document Frequency\n",
    "# Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples.\n",
    "# Word with their corresponding id\n",
    "corpus = [id2word.doc2bow(text) for text in tokenized_content_lemmatized]\n",
    "\n",
    "# View\n",
    "# print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge of using LDA for Topic Modeling is determining the right amount of topics *n*. Therefore we will use two metrics: Perplexity and Coherence as well as the pyLDAvis to approximately determine the right number of topics that are latent in our corpus (description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build list of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topics(num_topics):\n",
    "    list_models=[]\n",
    "    for n in num_topics:\n",
    "        topic_name = 'lda_model_' + str(n)\n",
    "        topic_name = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, \n",
    "                                                     num_topics=n, random_state=100, \n",
    "                                                     update_every=1, chunksize=100, \n",
    "                                                     passes=10, alpha='auto', \n",
    "                                                     per_word_topics=True)\n",
    "        list_models.append(topic_name)\n",
    "    return list_models\n",
    "\n",
    "num_topics = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "models = build_topics(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the models to disc\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "for i in range(len(models)):\n",
    "    temp_file = datapath(\"model\" + str(i))\n",
    "    models[i].save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "# lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to (Jansen, 2018) \"Perplexity when applied to LDA meassures how well the topic-word probability distribution recovered by the model predicts a sample, for example, unseen text documents. It is based on the entropy H(p) of this distribution p and computed with respect to the set of tokens w. Measures closer to zero imply the distribution is better at predicting the sample\".\n",
    "\n",
    "In terms of Coherence, we will be using Gensim implementation based on the paper writen by (Roder, et al., 2015). Larger scores in coherence mean better topic representation of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "Jansen, S. (2018). Perplexity. In Hands-on machine learning for algorithmic trading: Design and implement investment strategies based on smart algorithms that learn from data using Python. Birmingham: Packt Publishing.\n",
    "\n",
    "Roder, M., Both, A., and Hinneburg, A. (2015). Exploring the Space of Topic Coherence Measures. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM ’15, pages 399–408, New York, NY, USA. ACM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perplexity_coherence(models):\n",
    "    list_perplexity = []\n",
    "    list_coherence = []\n",
    "    \n",
    "    for model in models:\n",
    "        list_perplexity.append(model.log_perplexity(corpus))\n",
    "        coherence_model_lda = CoherenceModel(model=model, texts=tokenized_content_lemmatized, \n",
    "                                             dictionary=id2word, coherence='c_v')\n",
    "        list_coherence.append(coherence_model_lda.get_coherence())\n",
    "    return list_perplexity, list_coherence\n",
    "\n",
    "x = calc_perplexity_coherence(models)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(list(x)).transpose()\n",
    "df_metrics.columns = ['Perplexity','Coherence']\n",
    "df_metrics['Number of topics'] = num_topics\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphic of number of Topics and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity needs to be as low as possible \n",
    "plt.plot( 'Number of topics', 'Perplexity', data=df_metrics, color='skyblue')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphic of number of Topics and Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity needs to be as low as possible \n",
    "plt.plot( 'Number of topics', 'Coherence', data=df_metrics, color='orange')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topics accordint to the best *n*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_5 = pyLDAvis.gensim.prepare(models[3], corpus, id2word)\n",
    "vis_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_5, '../data/visualization_5_topics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_7 = pyLDAvis.gensim.prepare(models[5], corpus, id2word)\n",
    "vis_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_7, '../data/visualization_7_topics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis_9 = pyLDAvis.gensim.prepare(models[7], corpus, id2word)\n",
    "vis_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_9, '../data/visualization_9_topics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrame with scores and topics for 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns data frame with \n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=lst_texts):\n",
    "    # Init output\n",
    "    sent_topics_df = list()\n",
    "#     df_model_en_desc.reset_index(inplace=True)\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "#         print(row)\n",
    "        sent_topics_df.append(row)\n",
    "        \n",
    "    sent_topics_df = pd.DataFrame(sent_topics_df)\n",
    "    sent_topics_df.columns = ['Dominant_Topic_1', 'Dominant_Topic_2', 'Dominant_Topic_3','Dominant_Topic_4','Dominant_Topic_5']\n",
    "    sent_topics_df = pd.concat([df_model_en_desc['id'],sent_topics_df], axis=1)\n",
    "    df_n_cols = sent_topics_df.shape[1]\n",
    "    \n",
    "    return sent_topics_df, df_n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df, df_n_cols = format_topics_sentences(ldamodel=models[3], corpus=corpus, texts=lst_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df = sent_topics_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the Topic from its score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,df_n_cols):\n",
    "    top_col = 'topic' + str(i)\n",
    "    score_col = 'score_dom_topic_' + str(i)\n",
    "    sent_topics_df[top_col] = pd.DataFrame(sent_topics_df.iloc[:,i].tolist(), index=sent_topics_df.index)[0]\n",
    "    sent_topics_df[score_col] = pd.DataFrame(sent_topics_df.iloc[:,i].tolist(), index=sent_topics_df.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with tuples\n",
    "cols_2_drop = ['Dominant_Topic_1', 'Dominant_Topic_2','Dominant_Topic_3','Dominant_Topic_4','Dominant_Topic_5']\n",
    "sent_topics_df.drop(columns=cols_2_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting DataFrame\n",
    "sent_topics_df.rename(columns={\"topic1\": \"winner_topic\",\n",
    "                               \"score_dom_topic_1\":\"winner_topic_score\",\n",
    "                               \"topic2\": \"second_place_topic\",\n",
    "                               \"score_dom_topic_2\":\"second_topic_score\",\n",
    "                               \"topic3\": \"third_place_topic\",\n",
    "                               \"score_dom_topic_3\":\"third_topic_score\",\n",
    "                               \"topic4\": \"fourth_place_topic\",\n",
    "                               \"score_dom_topic_4\":\"fourth_topic_score\",\n",
    "                              \"topic5\": \"fifth_place_topic\",\n",
    "                               \"score_dom_topic_5\":\"fifth_topic_score\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the initial DataFrame with features with the topics DataFrame\n",
    "df_model_en_desc_merged = pd.merge(left=df_model_en_desc, right=sent_topics_df, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_en_desc_merged.iloc[:,0:6].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_en_desc_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_en_desc_merged = df_model_en_desc_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_en_desc_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the topic that appears as winner the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_model_en_desc_merged.groupby('winner_topic')['winner_topic'].size()\n",
    "print(df)\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model_en_desc_merged.iloc[:, 1:]\n",
    "X = X.loc[:, X.columns != 'price']\n",
    "X = X.loc[:, X.columns != 'content']\n",
    "X = X.loc[:, X.columns != 'content_lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_model_en_desc_merged.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split train - test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features to use as labels in FeatureImportances\n",
    "features=list()\n",
    "\n",
    "for col in df_model_en_desc_merged.columns:\n",
    "    features.append(col)\n",
    "\n",
    "# Removing price and id from labels\n",
    "features.remove('price')\n",
    "features.remove('content')\n",
    "features.remove('content_lang')\n",
    "features.remove('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, len(features), y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Cross-Validation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=12, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating multiple Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    \"support vector machine\": SVR(),\n",
    "    \"multilayer perceptron\": MLPRegressor(),\n",
    "    \"nearest neighbors\": KNeighborsRegressor(),\n",
    "    \"bayesian ridge\": BayesianRidge(),\n",
    "    \"linear regression\": LinearRegression(),\n",
    "    \"random forest\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "for _, regressor in regressors.items():\n",
    "#     visualizer = ResidualsPlot(regressor)\n",
    "#     visualizer.fit(X_train, y_train)\n",
    "#     visualizer.score(X_test, y_test)\n",
    "#     visualizer.show()\n",
    "    # Instantiate the classification model and visualizer\n",
    "\n",
    "    visualizer = CVScores(regressor, cv=cv)\n",
    "    visualizer.fit(X, y)\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance of Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(regressors['random forest'], labels=features, size=(1080, 1080))\n",
    "viz.fit(X_train, y_train)\n",
    "\n",
    "# Note: the FeatureImportances visualizer is a model visualizer,\n",
    "# not a feature visualizer, so it doesn't have a transform method!\n",
    "viz.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capst_kernel",
   "language": "python",
   "name": "capst_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
