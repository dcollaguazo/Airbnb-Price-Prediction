{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielacollaguazo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from re import sub\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "# import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from yellowbrick.features import rank2d\n",
    "from yellowbrick.classifier import confusion_matrix\n",
    "from yellowbrick.classifier import classification_report\n",
    "from yellowbrick.regressor import prediction_error, ResidualsPlot\n",
    "\n",
    "# Change pandas viewing options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing listings dataset\n",
    "df_listings = pd.read_csv('../data/new-york-city-airbnb-open-data/listings.csv')\n",
    "\n",
    "# importing the reviews text with polarity\n",
    "df_reviews_w_polarity = pd.read_csv('../variable_exploration/dc/output/reviews_with_sentiment_and_lang.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting reviews only in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting only reviews in English\n",
    "df_rev_eng=df_reviews_w_polarity[df_reviews_w_polarity.review_lang=='en']\n",
    "\n",
    "# slicing DF\n",
    "# df_rev_pol = df_rev_eng.loc[:,['id','listing_id','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    1009583\n",
       "Name: review_lang, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev_eng.review_lang.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1009583, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev_eng.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming id column\n",
    "df_listings.rename(columns={\"id\": \"listing_id\"}, inplace=True)\n",
    "\n",
    "# formatting price to float values\n",
    "df_listings.price = [float(sub(r'[^\\d.]', '', r['price'])) for i, r in df_listings.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings = df_listings[['listing_id', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_eng = df_rev_eng[['listing_id', 'comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging colums wit normalized ratings and the price column\n",
    "df_temp = pd.merge(left=df_rev_eng, right=df_listings, on='listing_id')\n",
    "\n",
    "# remove null values\n",
    "df_temp = df_temp.dropna()\n",
    "\n",
    "df_temp = df_temp.set_index('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>very nice neighborhood,close enough to \"A\" tra...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>I've stayed with my friend at the Midtown Cast...</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>We've been staying here for about 9 nights, en...</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>We had a wonderful stay at Jennifer's charming...</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>Hi to everyone!\\r\\nWould say our greatest comp...</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     comments  price\n",
       "listing_id                                                          \n",
       "2060        very nice neighborhood,close enough to \"A\" tra...  100.0\n",
       "2595        I've stayed with my friend at the Midtown Cast...  225.0\n",
       "2595        We've been staying here for about 9 nights, en...  225.0\n",
       "2595        We had a wonderful stay at Jennifer's charming...  225.0\n",
       "2595        Hi to everyone!\\r\\nWould say our greatest comp...  225.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(s):\n",
    "#     print(s)\n",
    "#     Replace special characters with ''\n",
    "    stripped = re.sub('[^\\w\\s]','',s)\n",
    "    stripped = re.sub('_','',stripped)\n",
    "    \n",
    "#     Change any whitespace to one space\n",
    "    stripped = re.sub('\\s',' ', stripped)\n",
    "    \n",
    "#     Remove numbers\n",
    "    stripped = re.sub('[\\d]','',stripped)\n",
    "    stripped = stripped.lower()\n",
    "#     To lowercase\n",
    "    \n",
    "#     Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['cleaned_comments'] = df_temp.apply(lambda row: text_cleaner(row['comments']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.to_csv('df_temp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp['tokenized_comments'] = df_temp.apply(lambda row: nltk.word_tokenize(row['cleaned_comments']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp['tokenized_comments_nostop'] = df_temp['tokenized_comments'].apply(lambda x:\n",
    "#                                                                                   [item for item in x \n",
    "#                                                                                    if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp['comments_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('df_temp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train, Xr_test, yr_train, yr_test = tts(df_temp.cleaned_comments, df_temp.price, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([Xr_train, yr_train], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([Xr_test, yr_test], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((807666, 2), (201917, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape , test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145214    i stayed at olivias for a pretty long time it ...\n",
       "201031    great fairly central location thats super easy...\n",
       "493775    is nice location marcs was very friendly excel...\n",
       "557839    my weekend stay in ny was complemented with a ...\n",
       "309164    the apartment is exactly as listed  very cute ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing terms that appear in more than 80%\n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, ngram_range=(4,4), \n",
    "                                 stop_words='english', lowercase=False, max_df=0.8, min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(df_temp.cleaned_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55468"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = tfidf_vectorizer_vectors[0:100000]\n",
    "X_1 = tfidf_vectorizer_vectors[100001:200000]\n",
    "X_2 = tfidf_vectorizer_vectors[200001:300000]\n",
    "X_3 = tfidf_vectorizer_vectors[300001:400000]\n",
    "X_4 = tfidf_vectorizer_vectors[400001:500000]\n",
    "X_5 = tfidf_vectorizer_vectors[500001:600000]\n",
    "X_6 = tfidf_vectorizer_vectors[600001:700000]\n",
    "X_7 = tfidf_vectorizer_vectors[700001:800000]\n",
    "X_8 = tfidf_vectorizer_vectors[800001:900000]\n",
    "X_9 = tfidf_vectorizer_vectors[900001:1009583]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0=pd.DataFrame(X_0.todense())\n",
    "df_1=pd.DataFrame(X_1.todense())\n",
    "df_2=pd.DataFrame(X_2.todense())\n",
    "df_3=pd.DataFrame(X_3.todense())\n",
    "df_4=pd.DataFrame(X_4.todense())\n",
    "df_5=pd.DataFrame(X_5.todense())\n",
    "df_6=pd.DataFrame(X_6.todense())\n",
    "df_7=pd.DataFrame(X_7.todense())\n",
    "df_8=pd.DataFrame(X_8.todense())\n",
    "df_9=pd.DataFrame(X_9.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to get the top terms for each review\n",
    "Source: https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_idx_sparse(matrix, n):\n",
    "    '''Return index of top n values in each row of a sparse matrix'''\n",
    "    top_n_idx = []\n",
    "    for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n",
    "        n_row_pick = min(n, ri - le)\n",
    "        top_n_idx.append(matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]])\n",
    "    return top_n_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44456, 224405,  21904,  40234,  34087, 213353,  83353, 236094,\n",
       "       192935,  22546], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_idx_sparse(X_0,10)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
