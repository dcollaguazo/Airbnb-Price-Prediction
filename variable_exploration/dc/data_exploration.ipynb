{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Change pandas viewing options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings = pd.read_csv(\"../../data/new-york-city-airbnb-open-data/listings.csv\")\n",
    "df_neighborhoods = pd.read_csv(\"../../data/new-york-city-airbnb-open-data/neighbourhoods.csv\")\n",
    "df_reviews = pd.read_csv(\"../../data/new-york-city-airbnb-open-data/reviews.csv\")\n",
    "df_calendar = pd.read_csv(\"../../data/new-york-city-airbnb-open-data/calendar.csv\")\n",
    "df_listings.rename(columns = {'id':'listing_id'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Listings Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings.shape\n",
    "# list(df_listings.columns)\n",
    "# df_listings.describe()\n",
    "# df_listings.dtypes\n",
    "# df_listings.head()\n",
    "# list(df_listings.columns)\n",
    "df_listings.last_scraped.value_counts()\n",
    "# df_listings.host_response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns with majority NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df_listings.isna().sum() * 100 / len(df_listings)\n",
    "missing_value_df = pd.DataFrame({'column_name': df_listings.columns,\n",
    "                                 'percent_missing': percent_missing.astype('int64')}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring missing values percentage \n",
    "# missing_value_df.iloc[21:40,:]\n",
    "missing_value_df['percent_missing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of columns to be dropped. In this case all of those with more than 30% of missing values\n",
    "columns_with_nulls_drop = list(missing_value_df[missing_value_df['percent_missing']>50]['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nulls_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with X% of missing values \n",
    "df_listings.drop(columns_with_nulls_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding true and false values\n",
    "df_listings.replace({'f': 0, 't': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings.loc[:,['last_scraped','host_since',\n",
    "                 'calendar_last_scraped','first_review','last_review']] = df_listings.loc[:,['last_scraped','host_since',\n",
    "                 'calendar_last_scraped','first_review','last_review']].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings.iloc[0:20,:].dtypes\n",
    "df_listings.listing_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating columns for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_for_NLP = df_listings.select_dtypes(include=['object'])\n",
    "df_listings_non_text = df_listings.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "host_listings_count and host_total_listings_count are always the same except in 5 cases where they are NaN. Therefore\n",
    "those columns will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum((df_listings_non_text['host_listings_count'] \n",
    "           == df_listings_non_text['host_total_listings_count']) \n",
    "          == False))\n",
    "\n",
    "df_listings_non_text.loc[((df_listings_non_text.host_listings_count \n",
    "                           == df_listings_non_text.host_total_listings_count) \n",
    "                          == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_non_text.drop(['host_listings_count','host_total_listings_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of numerical and boolean categories\n",
    "df_listings_non_text.hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with a single category\n",
    "df_listings_non_text.drop(['has_availability', 'host_has_profile_pic', \n",
    "                      'is_business_travel_ready', 'require_guest_phone_verification', \n",
    "                      'require_guest_profile_picture', 'requires_license'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_since**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings_non_text.host_since.value_counts()\n",
    "df_listings_non_text.host_since.head()\n",
    "\n",
    "# Calculating the number of days a host has been active to scraping date\n",
    "df_listings_non_text['host_days_active'] = df_listings_non_text.loc[:,'host_since'].apply(lambda x: \n",
    "                                                                                          datetime.datetime(\n",
    "                                                                                              2020, 3, 14) - x)\n",
    "\n",
    "# Printing mean and median\n",
    "print(\"Mean days as host:\", df_listings_non_text['host_days_active'].mean().days)\n",
    "print(\"Median days as host:\", df_listings_non_text['host_days_active'].median().days)\n",
    "\n",
    "# Replacing null values with the median\n",
    "df_listings_non_text.host_days_active.fillna(df_listings_non_text.host_days_active.median().days, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of review comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews.shape\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates compound sentiment polarity of the sentence\n",
    "vader_polarity_compound = lambda x: (SentimentIntensityAnalyzer().polarity_scores(x))['compound']\n",
    "\n",
    "# We can retrieve scores for positive, negative or neutral sentiment. \n",
    "# We will use the compound: a normalized value: norm_score = score / math.sqrt((score * score) + alpha)\n",
    "print(SentimentIntensityAnalyzer().polarity_scores('VADER is smart, handsome, and funny.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the number of no comments\n",
    "\"{}% of reviews have empty comments\".format((df_reviews.comments.isnull().sum() / df_reviews.comments.shape[0]) *100)\n",
    "# \"My name is {}, I'am {}\".format(\"John\",36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the percentage is so insignificant, these rows will be dropped\n",
    "df_reviews.dropna(subset=['comments'], how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pre-trained Vader sentiment model based on NLTK go create polarity scores for all reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['polarity'] = df_reviews.comments.map(vader_polarity_compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that predicts the language. It needs to be passed a string with decent amount of characters, thus the calculation on the fly of the lenght of the string passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang(x):\n",
    "    lang=''\n",
    "    txt_len=len(x)\n",
    "    if txt_len>100:\n",
    "        try:\n",
    "            lang=detect(x)\n",
    "        except Exception as e:\n",
    "            lang=''\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_lang'] = df_reviews.comments.apply(lambda x: predict_lang(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that a lot of reviews didnt get a language value. This is because their length was too short for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviews.review_lang.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews were exported for both English and Spanish. I checked the spanish reviews fo sentiment and wasn't accurate what leads me to believe that this algorithm works best for English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[df_reviews.review_lang == 'en'].to_csv('reviews_with_sentiment_en.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_response_time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_for_NLP.host_response_time.value_counts(), df_listings_for_NLP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in host response time:\", df_listings_for_NLP.host_response_time.isna().sum())\n",
    "print(f\"Proportion to the hosts WITHOUT response time: {round((df_listings_for_NLP.host_response_time.isna().sum()/len(df_listings_for_NLP))*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a connection between hosts not having response time and the reviews they get in terms of communication**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows without a value for host_response_time which have also not yet had a review\n",
    "df_host_response_review = pd.concat([df_listings_for_NLP['host_response_time'],\n",
    "                                     df_listings_non_text['first_review']], axis=1)\n",
    "print(\"Hosts with no response time that don't have a review yet:\",\n",
    "      len(df_host_response_review[df_host_response_review.loc[ :,\n",
    "                                                              ['host_response_time ','first_review']\n",
    "                                                             ].isnull().sum(axis=1) == 2]))\n",
    "\n",
    "print(\"Proportion to total hosts with no response time:\", df_host_response_review.host_response_time.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings_non_text.review_scores_communication.value_counts()\n",
    "# df_listings_non_text.review_scores_communication.isna().sum()\n",
    "\n",
    "tmp = df_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=tmp.loc[:,['host_response_time','first_review','review_scores_communication']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.host_response_time.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a correlation between response rate an a positive review?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calendar_last_scraped                           datetime64[ns]\n",
    "# number_of_reviews                                        int64\n",
    "# number_of_reviews_ltm                                    int64\n",
    "# first_review                                            object\n",
    "# last_review                                     datetime64[ns]\n",
    "# review_scores_rating                                   float64\n",
    "# review_scores_accuracy                                 float64\n",
    "# review_scores_cleanliness                              float64\n",
    "# review_scores_checkin                                  float64\n",
    "# review_scores_communication                            float64\n",
    "# review_scores_location                                 float64\n",
    "# review_scores_value                                    float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there correlation between starring and host response rate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "df_listings_for_NLP.host_response_time.fillna(\"unknown\", inplace=True)\n",
    "df_listings_for_NLP.host_response_time.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of unique listings in calendar and listings DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['listing_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining calendar with listings. I want to know data from what years are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_listings_date = pd.merge(df_listings, df_calendar, how='inner', on=['listing_id', 'listing_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['date'] = pd.to_datetime(df_calendar['date'])\n",
    "df_calendar ['year'] = pd.DatetimeIndex(df_calendar['date']).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Years of listings available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Categorical(df_calendar['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating season label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['season'] = (pd.DatetimeIndex(df_calendar['date']).month%12 + 3)//3\n",
    "df_calendar['season_l'] = pd.cut(df_calendar['season'], 4, labels=[\"winter\", \"spring\", \"summer\",\"autum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_calendar[df_calendar['season']==1]\n",
    "df_calendar.head()\n",
    "# df_calendar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the difference between adjusted price and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = [sub(r'[^\\d.]', '', r['price']) for i, r in df_calendar_temp.iterrows()]\n",
    "df_calendar['price'] = [float(sub(r'[^\\d.]', '', r['price'])) for i, r in df_calendar.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['adjusted_price'] = [float(sub(r'[^\\d.]', '', r['adjusted_price'])) for i, r in df_calendar.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['difference_price_adjusted'] = df_calendar['price'] - df_calendar['adjusted_price'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_calendar[df_calendar['difference_price_adjusted']<0]\n",
    "# conclusion: there is a difference between adjusted price and actual price. We need to further investigate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which areas of have the most Airbnb properties, and which are the most expensive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar[df_calendar['difference_price_adjusted']<0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTWorkshops",
   "language": "python",
   "name": "gtworkshops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
